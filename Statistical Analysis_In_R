### R 코드입니다. ###

#필요한 패키지 
library(caret) # model tuning #
library(klaR) # Naive Bayes #
library(pROC) # ROC curve #
library(party)
library(dplyr)
library(MASS)
library(rpart)
library(e1071)
library(clustMixType)
library("dplyr")

#BASE LINE
setwd('C:/Users/user/Desktop/')
dataset<-read.csv('Total.csv',header=TRUE)
dataset<-dataset[,-c(1:19,21)]
levels(dataset$y)<-c(NA,NA,NA,NA,'미입양','미입양','입양','미입양')
levels(dataset$y)<-c(0,1)
dataset_1<-dataset[-which(is.na(dataset$y)),]
dataset_1<-dataset_1[which(rowSums(is.na(dataset_1))==0),]

#의사결정나무
rpartmod<-rpart(y~. , data=dataset_1, method="class")
plot(rpartmod)
text(rpartmod)
plotcp(rpartmod)
ptree<-prune(rpartmod, cp= rpartmod$cptable[which.min(rpartmod$cptable[,"xerror"]),"CP"])
plot(ptree)
text(ptree)

dataset_1$Node<-ptree$where
dataset_1[which(dataset_1$Node%in%c(2,4)),c('cluster')]<-0
dataset_1[which(dataset_1$Node%in%c(5)),c('cluster')]<-1

#clust0 : 의사결정나무 결과 입양확률이 낮은 그룹
#clust1 : 의사결정나무 결과 입양확률이 높은 그룹

clust0<-dataset_1[which(dataset_1$cluster==0),]
clust1<-dataset_1[which(dataset_1$cluster==1),]

#clust0 (의사결정나무 결과 입양확률이 낮은 그룹 분석)

clust0$y<-as.numeric(clust0$y)
clust0[clust0$y==1,c('y')]<-0 #상태변수가 미입양인 경우
clust0[clust0$y==2,c('y')]<-1 #상태변수가 입양인 경우
clust0<-clust0[,-c(9:10)]

set.seed(20190912)
ind <- sample(2,nrow(clust0), replace = TRUE, prob = c(0.7,0.3))
training <- clust0[ind==1,] 
test <- clust0[ind==2,]

mean.training <- apply(training[,5:6], 2, mean)
sd.training <- apply(training[,5:6], 2, sd)
training[,5:6] <- (training[,5:6] - matrix(mean.training, nrow(training), 2, byrow=T)) / matrix(sd.training, nrow(training), 2, byrow=T)
test[,5:6] <- (test[,5:6] - matrix(mean.training, nrow(test), 2, byrow=T)) / matrix(sd.training, nrow(test), 2, byrow=T)
training$y<-as.factor(training$y)
test$y<-as.factor(test$y)

#로지스틱 회귀분석 수행(그룹0)
lrFit <- glm(y ~ ., data=training, family=binomial)
lrFit_1 <- stepAIC(lrFit,direction='both',trace=F)
lrPred1 <- predict(lrFit_1, test, type='response')	
lrPred2 <- factor(ifelse(lrPred1 > 0.5, '1', '0'))
lrCM <- confusionMatrix(lrPred2, test$y)
lrCM 
lrRoc <- roc(response=test$y, predictor=lrPred1, levels=rev(levels(test$y)))
lrRoc
auc(lrRoc)
ci.auc(lrRoc)
lr_non_result <- round(c('threshold'=0.5, lrCM$overall[1], lrCM$byClass[1:2], 'AUC'=auc(lrRoc), '2.5%'=ci.auc(lrRoc)[1], '97.5%'=ci.auc(lrRoc)[3]), 3)
lr_non_result

plot(lrRoc, cex.axis=1.5, cex.lab=2, xlab='Specificity', ylab='Sensitivity')
points(lrCM$byClass[2], lrCM$byClass[1], pch=16, cex=1.5,col='red')

#clust1 (의사결정나무 결과 입양확률이 높은 그룹 분석)

clust1$y<-as.numeric(clust1$y)
clust1[clust1$y==1,c('y')]<-0 #상태변수가 미입양인 경우
clust1[clust1$y==2,c('y')]<-1 #상태변수가 입양인 경우
clust1<-clust1[,-c(9:10)]

set.seed(20190912)
ind <- sample(2,nrow(clust1), replace = TRUE, prob = c(0.7,0.3))
training <- clust1[ind==1,] 
test <- clust1[ind==2,]

mean.training <- apply(training[,5:6], 2, mean)
sd.training <- apply(training[,5:6], 2, sd)
training[,5:6] <- (training[,5:6] - matrix(mean.training, nrow(training), 2, byrow=T)) / matrix(sd.training, nrow(training), 2, byrow=T)
test[,5:6] <- (test[,5:6] - matrix(mean.training, nrow(test), 2, byrow=T)) / matrix(sd.training, nrow(test), 2, byrow=T)
training$y<-as.factor(training$y)
test$y<-as.factor(test$y)

#로지스틱 회귀분석 수행(그룹1)
lrFit <- glm(y ~ ., data=training, family=binomial)
lrFit_1 <- stepAIC(lrFit,direction='both',trace=F)
lrPred1 <- predict(lrFit_1, test, type='response')	
lrPred2 <- factor(ifelse(lrPred1 > 0.5, '1', '0'))
lrCM <- confusionMatrix(lrPred2, test$y)
lrCM 
lrRoc <- roc(response=test$y, predictor=lrPred1, levels=rev(levels(test$y)))
lrRoc
auc(lrRoc)
ci.auc(lrRoc)
lr_non_result <- round(c('threshold'=0.5, lrCM$overall[1], lrCM$byClass[1:2], 'AUC'=auc(lrRoc), '2.5%'=ci.auc(lrRoc)[1], '97.5%'=ci.auc(lrRoc)[3]), 3)
lr_non_result

plot(lrRoc, cex.axis=1.5, cex.lab=2, xlab='Specificity', ylab='Sensitivity')
points(lrCM$byClass[2], lrCM$byClass[1], pch=16, cex=1.5,col='red')
